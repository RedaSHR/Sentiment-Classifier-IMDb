{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"logistic_regression-pretreatement.ipynb","provenance":[]},"deepnote":{"is_reactive":false},"deepnote_execution_queue":[],"deepnote_notebook_id":"c88d5560-6b93-4bd2-a182-a7b67e2c0f28","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oCqAwuC-a9WT"},"source":["## Logistic Regression Pretreatement"]},{"cell_type":"markdown","metadata":{"id":"Akx_vSvFUMUr"},"source":["We set the random seed to make our result reproductible."]},{"cell_type":"code","metadata":{"id":"cE24x_bwULfG","executionInfo":{"status":"ok","timestamp":1634139131519,"user_tz":-120,"elapsed":202,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["import random\n","\n","random.seed(10)"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uKmgaKdJUS-g"},"source":["First we import everything we need for this sheet."]},{"cell_type":"code","metadata":{"id":"FrPWLL31Hv00","executionInfo":{"status":"ok","timestamp":1634139131703,"user_tz":-120,"elapsed":3,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["# import datasets\n","from datasets import load_dataset, concatenate_datasets\n","import pandas as pd"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZS99owPXbb9A"},"source":["We download the dataset from HuggingFace. We will manually split data train and test set. First we will going to merge the train and test dataset into one dataset of 50 000 elements. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72bZGI1IHv08","executionInfo":{"status":"ok","timestamp":1634139132671,"user_tz":-120,"elapsed":969,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"70cc1804-f4c6-40a1-b3ab-d0818e01d9f2"},"source":["dataset_train = load_dataset('imdb', split='train')\n","dataset_test = load_dataset('imdb', split='test')"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n","Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"]}]},{"cell_type":"code","metadata":{"id":"2YeW7tzMNGc-","executionInfo":{"status":"ok","timestamp":1634139132671,"user_tz":-120,"elapsed":3,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["dataset = concatenate_datasets([dataset_train, dataset_test])"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5LFe_8_IUwgD"},"source":["Now that we have our data, we want to convert it to a DataFrame to facilitate manipulations."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"KNeMp2fyNGc_","executionInfo":{"status":"ok","timestamp":1634139132951,"user_tz":-120,"elapsed":283,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"33b80b36-7630-4ff1-ab16-8043634b8334"},"source":["from typing import List, Tuple\n","\n","def create_dataframe(data: List[Tuple[str, str]], columns: List[str]) -> pd.DataFrame:\n","    \"\"\" Convert our data into a DataFrame and convert the string identifier to int \"\"\"\n","\n","    rtn = pd.DataFrame(data, columns=columns)\n","    return rtn\n","\n","df = create_dataframe(list(zip(dataset['label'], dataset['text'])), ['Label', 'Text'])\n","df.head()"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Homelessness (or Houselessness as George Carli...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>This is easily the most underrated film inn th...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>This is not the typical Mel Brooks film. It wa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Label                                               Text\n","0      1  Bromwell High is a cartoon comedy. It ran at t...\n","1      1  Homelessness (or Houselessness as George Carli...\n","2      1  Brilliant over-acting by Lesley Ann Warren. Be...\n","3      1  This is easily the most underrated film inn th...\n","4      1  This is not the typical Mel Brooks film. It wa..."]},"metadata":{},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"fDygW1gwHv1B","executionInfo":{"status":"ok","timestamp":1634139132952,"user_tz":-120,"elapsed":6,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["# import packages for steeming\n","import nltk\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.tokenize import word_tokenize"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFV8G6diHv1C","executionInfo":{"status":"ok","timestamp":1634139133212,"user_tz":-120,"elapsed":266,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"96e98b60-f2dd-444d-c07b-a912b40fc71e"},"source":["# We need to download a package for word tokenization\n","nltk.download('punkt')"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"GvjNoecW-Ags"},"source":["Let's start by removing html tags"]},{"cell_type":"code","metadata":{"id":"r1-atGiaHv1F","executionInfo":{"status":"ok","timestamp":1634139152211,"user_tz":-120,"elapsed":19002,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["# Remove html tags\n","from bs4 import BeautifulSoup\n","df['Text'] = df['Text'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text().strip())"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sVWDt3Jg-AD9"},"source":["Let's continue with the word tokenization."]},{"cell_type":"code","metadata":{"id":"uWC7zsizHv1G","executionInfo":{"status":"ok","timestamp":1634139235005,"user_tz":-120,"elapsed":82798,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["# Tokenization\n","df['Text'] = df['Text'].apply(lambda x: \" \".join(word_tokenize(x)))"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ar2BELNx-IM-"},"source":["Now let's apply the stemming to everything that is composed of characters. Words are simply cut and stemmed. We do not have any punctuation."]},{"cell_type":"code","metadata":{"id":"Xv3cs0pyHv1H","executionInfo":{"status":"ok","timestamp":1634139464218,"user_tz":-120,"elapsed":229218,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["# Steeming\n","import re\n","\n","re_word = re.compile(r\"^\\w+$\")\n","stemmer = SnowballStemmer(\"english\")\n","\n","def stemming(text):\n","    return [stemmer.stem(word) for word in word_tokenize(text.lower()) if re_word.match(word)]\n","        \n","df['Text'] = df['Text'].apply(lambda x: \" \".join(stemming(x)))"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44OY8ecRHv1I","executionInfo":{"status":"ok","timestamp":1634139464219,"user_tz":-120,"elapsed":11,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"418ff62a-2b8c-425b-d22a-71c32514b407"},"source":["df['Text'][:5]"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    bromwel high is a cartoon comedi it ran at the...\n","1    homeless or houseless as georg carlin state ha...\n","2    brilliant by lesley ann warren best dramat hob...\n","3    this is easili the most underr film inn the br...\n","4    this is not the typic mel brook film it was mu...\n","Name: Text, dtype: object"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mx6X-ZMtHv1J","executionInfo":{"status":"ok","timestamp":1634139470306,"user_tz":-120,"elapsed":6092,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"45ab78ac-0973-4f73-c655-ac6ffb986c6f"},"source":["!python -m spacy download en_core_web_sm"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"markdown","metadata":{"id":"D7zDmUr5dd40"},"source":["Let's lemmatize all token we can find."]},{"cell_type":"code","metadata":{"id":"sUiLRivcHv1K","executionInfo":{"status":"ok","timestamp":1634139533715,"user_tz":-120,"elapsed":63411,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["# Lemmatization\n","import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\", disable = ['ner', 'tagger', 'parser', 'textcat', \"lemmatizer\"])\n","\n","def lemmatization(text):\n","    return [token.lemma_ for token in nlp(text.lower()) if re_word.match(token.text)]\n","\n","df['Text'] = df['Text'].apply(lambda x: \" \".join(lemmatization(x)))"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yjkATqAib7dV"},"source":["First, we need to convert the text into numbers that we can do calculations on. We use word frequencies. We want to transform the given text to a vector on the basis of the frequency of each word in the text.\n","\n","For this we use `CountVectorizer` from `sklearn`."]},{"cell_type":"code","metadata":{"id":"fLWCv6pEHv1O","executionInfo":{"status":"ok","timestamp":1634139542176,"user_tz":-120,"elapsed":8466,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer(max_features = 1500)\n"," \n","X = cv.fit_transform(df['Text']).toarray()\n","y = df['Label']"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GfPUnLzvb_Ad"},"source":["The `train_test_split` shuffles all the dataset before splitting. In our case, we will use 75% of data for training and 25% for testing."]},{"cell_type":"code","metadata":{"id":"EeKg2muGIaK-","executionInfo":{"status":"ok","timestamp":1634139542467,"user_tz":-120,"elapsed":295,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(\n","           X, y, test_size = 0.25, random_state = 0)"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5YMvBXjHv1P","executionInfo":{"status":"ok","timestamp":1634139581945,"user_tz":-120,"elapsed":39480,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"ca903579-b970-44e8-de48-6acb16fbe9a4"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","logreg = LogisticRegression(max_iter=1000)\n","logreg.fit(X_train, y_train)"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"lascZojE-Q-3"},"source":["We use the confusion_matrix of sklearn to display the number of right (True positive and True negative) and wrong (False positive and False negative) predictions."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nGAyntwHv1R","executionInfo":{"status":"ok","timestamp":1634139582333,"user_tz":-120,"elapsed":397,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"08fe1ff3-4fda-4bba-e058-28eeb1fd9e69"},"source":["from sklearn.metrics import confusion_matrix\n","y_pred = logreg.predict(X_test)\n","cm = confusion_matrix(y_test, y_pred)\n","cm"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[5390,  862],\n","       [ 768, 5480]])"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"0-7VymI4-TQy"},"source":["We use the classification_report of sklearn to display the precision, recall, and F1-score for both classes on the test data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_F-smA6gHv1S","executionInfo":{"status":"ok","timestamp":1634139582334,"user_tz":-120,"elapsed":14,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"051431b6-0a9f-4515-ff90-84e31d1d073d"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred))"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.88      0.86      0.87      6252\n","           1       0.86      0.88      0.87      6248\n","\n","    accuracy                           0.87     12500\n","   macro avg       0.87      0.87      0.87     12500\n","weighted avg       0.87      0.87      0.87     12500\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Rkj_UPLL-Uv4"},"source":["Compared to logistic regression without pretreatement, we have quite the same accuracy, precision for both classes. Pretreatement on word frequencies features using logistic regression doesn't really have impact."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"EZJwEiE4_fIK","executionInfo":{"status":"ok","timestamp":1634139582335,"user_tz":-120,"elapsed":12,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"8c7a9674-48a1-4f94-f5d9-199ece845761"},"source":["bad_predict_df = y_test.where(y_test != y_pred).dropna()\n","indexes = bad_predict_df.index\n","df.iloc[indexes]"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12968</th>\n","      <td>0</td>\n","      <td>i see this movi a a veri young girl i 27 now a...</td>\n","    </tr>\n","    <tr>\n","      <th>43460</th>\n","      <td>0</td>\n","      <td>ray bradburi run and hide this tacki film vers...</td>\n","    </tr>\n","    <tr>\n","      <th>34478</th>\n","      <td>1</td>\n","      <td>this be a veri dark movi somewhat well than th...</td>\n","    </tr>\n","    <tr>\n","      <th>19883</th>\n","      <td>0</td>\n","      <td>i read comment that you should watch this film...</td>\n","    </tr>\n","    <tr>\n","      <th>2838</th>\n","      <td>1</td>\n","      <td>okay yes this be a movi with continu error lik...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11686</th>\n","      <td>1</td>\n","      <td>i probabl doubl my knowledg of iran when i see...</td>\n","    </tr>\n","    <tr>\n","      <th>34600</th>\n","      <td>1</td>\n","      <td>the ﻿1 time i see this film i want to like it ...</td>\n","    </tr>\n","    <tr>\n","      <th>21800</th>\n","      <td>0</td>\n","      <td>i ﻿1 see this movi on mst3k and although i lau...</td>\n","    </tr>\n","    <tr>\n","      <th>27615</th>\n","      <td>1</td>\n","      <td>sheba babi be alway underr much like becaus it...</td>\n","    </tr>\n","    <tr>\n","      <th>30528</th>\n","      <td>1</td>\n","      <td>good i guess i be in the mood for a movi that ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1630 rows × 2 columns</p>\n","</div>"],"text/plain":["       Label                                               Text\n","12968      0  i see this movi a a veri young girl i 27 now a...\n","43460      0  ray bradburi run and hide this tacki film vers...\n","34478      1  this be a veri dark movi somewhat well than th...\n","19883      0  i read comment that you should watch this film...\n","2838       1  okay yes this be a movi with continu error lik...\n","...      ...                                                ...\n","11686      1  i probabl doubl my knowledg of iran when i see...\n","34600      1  the ﻿1 time i see this film i want to like it ...\n","21800      0  i ﻿1 see this movi on mst3k and although i lau...\n","27615      1  sheba babi be alway underr much like becaus it...\n","30528      1  good i guess i be in the mood for a movi that ...\n","\n","[1630 rows x 2 columns]"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136},"id":"GmPkaqj6FC57","executionInfo":{"status":"ok","timestamp":1634139582335,"user_tz":-120,"elapsed":10,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"e22770fe-7379-4676-bbf8-6eac8a38a509"},"source":["df.iloc[12968][\"Text\"]"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'i see this movi a a veri young girl i 27 now and it scare me witless for year i have nightmar about everi aspect of this film from the way it be draw to the music to obvious the violenc my parent still argu about who allow me to watch it and both of them say that they would never let me watch such a movi i think they onli say that know that i have such strong feel about it 0 i be current read the book out of morbid curio and the fact that it a classic and it be realli a great stori howev i do think that it should have be make into a cartoon ever good mayb kid nowaday would find it quaint but it give me nightmar for week and week and i still have a hard time see rabbit draw in a similar way give me a littl heart palpit everi time yah i be a wuss but i strong suggest that anus parent look to show this movi to their kid read them the book instead or watch it \\ufeff1 to make certain that they approv of the content not everyon find it a disturb a i do but we be out there 0'"]},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"W_xgzWfLzJPx"},"source":["\n","\n","--------------------------------------------------------------------------\n","\n","\n","Now that we have created our model with logistic regression based on word frequencies using CountVectorizer, we will tried this time a model with logistic regression based on hand-made features."]},{"cell_type":"markdown","metadata":{"id":"jmNJz-CHJodA"},"source":["We have chosen to implement 4 features: presence of keyword \"no\" or not, presence of \"!\" or not, number of positive and negative keywords using vaderSentiment.txt. "]},{"cell_type":"code","metadata":{"id":"10gCfxOeRZah","executionInfo":{"status":"ok","timestamp":1634139582336,"user_tz":-120,"elapsed":10,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","import re\n","analyzer = SentimentIntensityAnalyzer()\n","\n","def countPositiveNegative(sentence: str) -> Tuple[str, str]:\n","  \"\"\" Returns number of positive and negative words passing a sentence in argument\"\"\"\n","  positive = 0\n","  negative = 0\n","  res = re.findall(r'\\w+', sentence) \n","  for word in res:\n","    vs = analyzer.polarity_scores(word)\n","    if vs['compound'] >= 0.05 :\n","        positive += 1\n","    elif vs['compound'] <= - 0.05 :\n","        negative += 1\n","  return positive, negative\n"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zip05O09hH-T","executionInfo":{"status":"ok","timestamp":1634139625649,"user_tz":-120,"elapsed":1590,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"5a7db31c-a7ae-49bb-edcd-a95ffabe2419"},"source":["import numpy as np\n","\n","def getAllFeatures(df : pd.DataFrame):\n","  \"\"\" Add new columns to dataframe df for features \"\"\"\n","  df['containsNo'] = np.where(df['Text'].str.contains(r\"\\b(no)\\b\", case = False), 1, 0)\n","  df['containsExclamation'] = np.where(df['Text'].str.contains(\"!\"), 1, 0)\n","\n","getAllFeatures(df)"],"execution_count":58,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n","  return func(self, *args, **kwargs)\n"]}]},{"cell_type":"code","metadata":{"id":"90vLku5kRjbx","executionInfo":{"status":"ok","timestamp":1634139804729,"user_tz":-120,"elapsed":173826,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["df = pd.concat([df, df['Text'].apply(lambda cell: pd.Series(countPositiveNegative(cell), index = ['positiveWords', 'negativeWords']))], axis = 1)"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"i6Bq86QfRkcB","executionInfo":{"status":"ok","timestamp":1634139804730,"user_tz":-120,"elapsed":24,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"7d1d6054-cd3c-40c0-cfd9-afd34ca359e1"},"source":["df"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Text</th>\n","      <th>containsNo</th>\n","      <th>containsExclamation</th>\n","      <th>positiveWords</th>\n","      <th>negativeWords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>bromwel high be a cartoon comedi it run at the...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>homeless or houseless a georg carlin state hav...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>19</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>brilliant by lesley ann warren well dramat hob...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>this be easili the much underr film inn the br...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>this be not the typic mel brook film it be muc...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>0</td>\n","      <td>i occasion let my kid watch this garbag so the...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>0</td>\n","      <td>when all we have anymor be pretti much realiti...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>0</td>\n","      <td>the basic genr be a thriller intercut with a u...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>0</td>\n","      <td>four thing intrigu me a to this film ﻿1 it sta...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>0</td>\n","      <td>david bryce comment nearbi be except good writ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 6 columns</p>\n","</div>"],"text/plain":["       Label  ... negativeWords\n","0          1  ...             0\n","1          1  ...            10\n","2          1  ...             4\n","3          1  ...             3\n","4          1  ...             2\n","...      ...  ...           ...\n","49995      0  ...             4\n","49996      0  ...             8\n","49997      0  ...             4\n","49998      0  ...             1\n","49999      0  ...             8\n","\n","[50000 rows x 6 columns]"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"Z34NWuiOgl0p","executionInfo":{"status":"ok","timestamp":1634139804731,"user_tz":-120,"elapsed":14,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["X = df.loc[:, ~df.columns.isin(['Text','Label'])]\n","y = df['Label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","           X, y, test_size = 0.25, random_state = 0)"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"z37Dz63Oii9m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634139804732,"user_tz":-120,"elapsed":15,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"0f54738d-dfc1-48be-cdc2-3a65f0ffe173"},"source":["logreg2 = LogisticRegression(max_iter=1000)\n","logreg2.fit(X_train, y_train)"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"MO0r4tFHioC7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634139805239,"user_tz":-120,"elapsed":517,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"fac05a74-795a-45f3-9194-3d8f33070790"},"source":["from sklearn.metrics import confusion_matrix\n","y_pred = logreg2.predict(X_test)\n","cm = confusion_matrix(y_test, y_pred)\n","cm"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[4181, 2071],\n","       [1967, 4281]])"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"bOPob_q9isxv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634139805241,"user_tz":-120,"elapsed":12,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"4f211dc2-ab08-44dd-edbd-f8fe43d85ebe"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred))"],"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.67      0.67      6252\n","           1       0.67      0.69      0.68      6248\n","\n","    accuracy                           0.68     12500\n","   macro avg       0.68      0.68      0.68     12500\n","weighted avg       0.68      0.68      0.68     12500\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"OXaLbhZpJ-OI"},"source":["Accuracy and precision of both classes using hand made features is not very good. Maybe the use of those features were not appropriate to the imdb dataset. Using vaderSentiment on all the sentence and not by counting only positive and negative words will have been surely more efficient."]},{"cell_type":"markdown","metadata":{"id":"3AoJgg1h-xYX"},"source":["In the case of pretreatement, steeming delete all punctuations in sentences.  The feature of presence of '!' or not is useless. "]}]}