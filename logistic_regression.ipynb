{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"deepnote":{"is_reactive":false},"deepnote_notebook_id":"d58e35ae-d0bf-46cb-9a34-70e3b6d485fe","deepnote_execution_queue":[],"colab":{"name":"logistic_regression.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qzV9A1PXUExs"},"source":["## Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"Akx_vSvFUMUr"},"source":["We set the random seed to make our result reproductible."]},{"cell_type":"code","metadata":{"id":"cE24x_bwULfG","executionInfo":{"status":"ok","timestamp":1634139034754,"user_tz":-120,"elapsed":326,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["import random\n","\n","random.seed(10)"],"execution_count":68,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uKmgaKdJUS-g"},"source":["First we import everything we need for this sheet."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"964784de","execution_start":1633969845102,"execution_millis":1772,"cell_id":"00000-83b4c1b3-30c4-4c8f-9253-d4c75d9ad9b9","id":"OCUOXQ5XNGc4","deepnote_cell_type":"code","executionInfo":{"status":"ok","timestamp":1634139034952,"user_tz":-120,"elapsed":3,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["# import datasets\n","from datasets import load_dataset, concatenate_datasets\n","import pandas as pd"],"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B81UdeGkbZv_"},"source":["We download the dataset from HuggingFacee. We will manually split data train and test set. First we will going to merge train and test dataset into one dataset of 50 000 elements. "]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"10854a6e","execution_start":1633969846887,"execution_millis":88751,"cell_id":"00002-ff4ce08b-d9fa-4f1a-99ba-127cc4980400","colab":{"base_uri":"https://localhost:8080/"},"id":"t-1-cqwcNGc9","deepnote_cell_type":"code","executionInfo":{"status":"ok","timestamp":1634139036405,"user_tz":-120,"elapsed":1456,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"09a1792c-f92f-416f-b826-f215d3d9b814"},"source":["dataset_train = load_dataset('imdb', split='train')\n","dataset_test = load_dataset('imdb', split='test')"],"execution_count":70,"outputs":[{"output_type":"stream","name":"stderr","text":["Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n","Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"]}]},{"cell_type":"code","metadata":{"id":"G4uEcoHv5ehF","executionInfo":{"status":"ok","timestamp":1634139036406,"user_tz":-120,"elapsed":5,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["dataset = concatenate_datasets([dataset_train, dataset_test])"],"execution_count":71,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5LFe_8_IUwgD"},"source":["Now that we have our data, we want to convert it to a DataFrame to facilitate manipulations."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"8ee28605","execution_start":1633969993621,"execution_millis":27,"cell_id":"00004-50e30939-3400-4790-ac9f-88e759db184d","id":"KNeMp2fyNGc_","deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"status":"ok","timestamp":1634139036878,"user_tz":-120,"elapsed":475,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"14a5490c-bbe5-4fcc-f07c-2c6405aaa830"},"source":["from typing import List, Tuple\n","\n","def create_dataframe(data: List[Tuple[str, str]], columns: List[str]) -> pd.DataFrame:\n","    \"\"\" Convert our data into a DataFrame and convert the string identifier to int \"\"\"\n","\n","    rtn = pd.DataFrame(data, columns=columns)\n","    return rtn\n","\n","df = create_dataframe(list(zip(dataset['label'], dataset['text'])), ['Label', 'Text'])\n","df.head()"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Homelessness (or Houselessness as George Carli...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>This is easily the most underrated film inn th...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>This is not the typical Mel Brooks film. It wa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Label                                               Text\n","0      1  Bromwell High is a cartoon comedy. It ran at t...\n","1      1  Homelessness (or Houselessness as George Carli...\n","2      1  Brilliant over-acting by Lesley Ann Warren. Be...\n","3      1  This is easily the most underrated film inn th...\n","4      1  This is not the typical Mel Brooks film. It wa..."]},"metadata":{},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"GCSZS-7LU4wy"},"source":["First, we need to convert the text into numbers that we can do calculations on. We use word frequencies. We want to transform the given text to a vector on the basis of the frequency of each word in the text.\n","\n","For this we use `CountVectorizer` from `sklearn`."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"cca4b111","execution_start":1633969993652,"execution_millis":11249,"cell_id":"00007-c54b8312-70e9-4027-b2bc-2f6ece67bd16","id":"hag3U5N2NGdB","deepnote_cell_type":"code","executionInfo":{"status":"ok","timestamp":1634139045774,"user_tz":-120,"elapsed":8899,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer(max_features = 1500)\n"," \n","X = cv.fit_transform(df['Text']).toarray()\n","y = df['Label']"],"execution_count":73,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lw2I9Wr7U9oH"},"source":["The `train_test_split` shuffles all the dataset before splitting. In our case, we will use 75% of data for training and 25% for testing."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"f43d1d72","execution_start":1633970004901,"execution_millis":14151,"cell_id":"00008-ae7c0a69-9018-46ac-b8ca-576a1c5a69a5","id":"5mbE48eZNGdC","deepnote_cell_type":"code","executionInfo":{"status":"ok","timestamp":1634139045774,"user_tz":-120,"elapsed":12,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(\n","           X, y, test_size = 0.25, random_state = 0)"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"40c2a25b","execution_start":1633964009984,"execution_millis":51154,"cell_id":"00009-84546ace-3698-4ed3-9c10-560c088be444","colab":{"base_uri":"https://localhost:8080/"},"id":"tGWlo1GnNGdD","deepnote_cell_type":"code","executionInfo":{"status":"ok","timestamp":1634139098315,"user_tz":-120,"elapsed":52551,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"8746a9b3-305e-430d-894a-6e1407fadfce"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","logreg = LogisticRegression(max_iter=1000)\n","logreg.fit(X_train, y_train)"],"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{},"execution_count":75}]},{"cell_type":"markdown","metadata":{"id":"eN-cEjrK8vbd"},"source":["We use the confusion_matrix of sklearn to display the number of right (True positive and True negative) and wrong (False positive and False negative) predictions."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"d9d311d0","execution_start":1633964061136,"execution_millis":654,"cell_id":"00010-decb8b68-aa72-4d6b-a269-6ad73ce48c63","colab":{"base_uri":"https://localhost:8080/"},"id":"E_yjea_cNGdD","deepnote_cell_type":"code","executionInfo":{"status":"ok","timestamp":1634139098316,"user_tz":-120,"elapsed":9,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"43a32e80-d9e9-4376-db94-5dcee436e7ab"},"source":["from sklearn.metrics import confusion_matrix\n","y_pred = logreg.predict(X_test)\n","cm = confusion_matrix(y_test, y_pred)\n","cm"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[5453,  799],\n","       [ 773, 5475]])"]},"metadata":{},"execution_count":76}]},{"cell_type":"markdown","metadata":{"id":"vNgmtHFZ8y0k"},"source":["We use the classification_report of sklearn to display the precision, recall, and F1-score for both classes on the test data."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"aae199db","execution_start":1633964061782,"execution_millis":63,"cell_id":"00011-f3b0893d-6661-45a6-bb77-338d957fb238","colab":{"base_uri":"https://localhost:8080/"},"id":"lX8SDwiGNGdD","deepnote_cell_type":"code","executionInfo":{"status":"ok","timestamp":1634139098621,"user_tz":-120,"elapsed":309,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"1c046db0-8ca5-40f4-e39a-15d392a2a94c"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred))"],"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.88      0.87      0.87      6252\n","           1       0.87      0.88      0.87      6248\n","\n","    accuracy                           0.87     12500\n","   macro avg       0.87      0.87      0.87     12500\n","weighted avg       0.87      0.87      0.87     12500\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"S1CD-xm184uf"},"source":["Compare to method using naive bayes model, we can see that logistic regression with word frequencies have better results than naive bayes model. This may be due to the assumptions that the Naive Bayes algorithm makes: independence assumption"]},{"cell_type":"markdown","metadata":{"id":"JayHVc7von6E"},"source":["\n","\n","--------------------------------------------------------------------------\n","\n","\n","Now that we have created our model with logistic regression based on word frequencies using CountVectorizer, we will tried this time a model with logistic regression based on hand-made features."]},{"cell_type":"markdown","metadata":{"id":"dteTKHjjSyBC"},"source":["We have chosen to implement 4 features: presence of keyword \"no\" or not, presence of \"!\" or not, number of positive and negative keywords using vaderSentiment.txt."]},{"cell_type":"code","metadata":{"id":"El9VFJ7joqrB","executionInfo":{"status":"ok","timestamp":1634139098622,"user_tz":-120,"elapsed":4,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","import re\n","analyzer = SentimentIntensityAnalyzer()\n","\n","def countPositiveNegative(sentence: str) -> Tuple[str, str]:\n","  \"\"\" Returns number of positive and negative words passing a sentence in argument\"\"\"\n","  positive = 0\n","  negative = 0\n","  res = re.findall(r'\\w+', sentence) \n","  for word in res:\n","    vs = analyzer.polarity_scores(word)\n","    if vs['compound'] >= 0.05 :\n","        positive += 1\n","    elif vs['compound'] <= - 0.05 :\n","        negative += 1\n","  return positive, negative"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5qsyQ_BotTj","executionInfo":{"status":"ok","timestamp":1634139100136,"user_tz":-120,"elapsed":1517,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"97581977-688b-4280-a5d8-f5393308554a"},"source":["import numpy as np\n","\n","def getAllFeatures(df: pd.DataFrame):\n","  df['containsNo'] = np.where(df['Text'].str.contains(r\"\\b(no)\\b\", case = False), 1, 0)\n","  df['containsExclamation'] = np.where(df['Text'].str.contains(\"!\"), 1, 0)\n","\n","getAllFeatures(df)"],"execution_count":79,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n","  return func(self, *args, **kwargs)\n"]}]},{"cell_type":"code","metadata":{"id":"awCzPygHPpPx","executionInfo":{"status":"ok","timestamp":1634139288646,"user_tz":-120,"elapsed":188513,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["df = pd.concat([df, df['Text'].apply(lambda cell: pd.Series(countPositiveNegative(cell), index = ['positiveWords', 'negativeWords']))], axis = 1)"],"execution_count":80,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"ZS3tkFFKR4Z6","executionInfo":{"status":"ok","timestamp":1634139289471,"user_tz":-120,"elapsed":7,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"fcdd3da4-a1c7-46e7-c325-75ca5ad91477"},"source":["df"],"execution_count":86,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Text</th>\n","      <th>containsNo</th>\n","      <th>containsExclamation</th>\n","      <th>positiveWords</th>\n","      <th>negativeWords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Homelessness (or Houselessness as George Carli...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>This is easily the most underrated film inn th...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>This is not the typical Mel Brooks film. It wa...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>0</td>\n","      <td>I occasionally let my kids watch this garbage ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>0</td>\n","      <td>When all we have anymore is pretty much realit...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>0</td>\n","      <td>The basic genre is a thriller intercut with an...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>0</td>\n","      <td>Four things intrigued me as to this film - fir...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>0</td>\n","      <td>David Bryce's comments nearby are exceptionall...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 6 columns</p>\n","</div>"],"text/plain":["       Label  ... negativeWords\n","0          1  ...             2\n","1          1  ...            13\n","2          1  ...             5\n","3          1  ...             3\n","4          1  ...             2\n","...      ...  ...           ...\n","49995      0  ...             6\n","49996      0  ...            10\n","49997      0  ...             7\n","49998      0  ...             2\n","49999      0  ...            10\n","\n","[50000 rows x 6 columns]"]},"metadata":{},"execution_count":86}]},{"cell_type":"code","metadata":{"id":"WAMa7CIMouig","executionInfo":{"status":"ok","timestamp":1634139288647,"user_tz":-120,"elapsed":10,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}}},"source":["X = df.loc[:, ~df.columns.isin(['Text','Label'])]\n","y = df['Label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","           X, y, test_size = 0.25, random_state = 0)"],"execution_count":81,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9PtPacyowFJ","executionInfo":{"status":"ok","timestamp":1634139289060,"user_tz":-120,"elapsed":421,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"84c3d688-da31-4aca-9a6f-a6dee4c5cbb5"},"source":["logreg2 = LogisticRegression(max_iter=1000)\n","logreg2.fit(X_train, y_train)"],"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"968CSBQOoxWt","executionInfo":{"status":"ok","timestamp":1634139289060,"user_tz":-120,"elapsed":11,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"f22ca1dd-8866-468b-ac31-a30b36f94199"},"source":["from sklearn.metrics import confusion_matrix\n","y_pred = logreg2.predict(X_test)\n","cm = confusion_matrix(y_test, y_pred)\n","cm"],"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[4401, 1851],\n","       [1838, 4410]])"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j91aWNrpoyfZ","executionInfo":{"status":"ok","timestamp":1634139289061,"user_tz":-120,"elapsed":10,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"1bd70860-52c2-478f-d65c-d4bd81dd1f70"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred))"],"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.71      0.70      0.70      6252\n","           1       0.70      0.71      0.71      6248\n","\n","    accuracy                           0.70     12500\n","   macro avg       0.70      0.70      0.70     12500\n","weighted avg       0.70      0.70      0.70     12500\n","\n"]}]}]}