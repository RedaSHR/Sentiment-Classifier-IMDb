{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"deepnote":{"is_reactive":false},"deepnote_notebook_id":"993acc26-f108-4cc4-8254-b3e90ae4645d","deepnote_execution_queue":[],"colab":{"name":"naive_bayers-pretreatement.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"p65TuYP7MPmb"},"source":["## Naive bayers pretreatement"]},{"cell_type":"markdown","metadata":{"id":"OHimS1khR1Fn"},"source":["We set the random seed to make our result reproductible."]},{"cell_type":"code","metadata":{"id":"Zv5MzkHrNllm"},"source":["import random\n","\n","random.seed(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ugXefPlGMbhp"},"source":["First we import everything we need for this sheet.\n","Torchtext includes several datasets. We will use IMDB dataset in our case."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"964784de","execution_start":1633961385785,"execution_millis":915,"cell_id":"00001-9e6e9c4a-c414-4553-9d24-272bc0cbb195","deepnote_cell_type":"code","id":"YRYZo_faAUQT"},"source":["# import datasets\n","from datasets import load_dataset, concatenate_datasets\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uO9l0qvwMkWD"},"source":["We download the data from HuggingFace. We will manually split data train and test set. First we will going to merge train and test dataset into one dataset of 50 000 elements. "]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"abf40245","execution_start":1633961402448,"execution_millis":47192,"cell_id":"00002-781cd9b8-178c-49da-a19c-b7823eb310a3","deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/","height":183,"referenced_widgets":["49075b162cb54745ac39f85387c7fb99","dd5bfc902c1640caacf19fdb70076f45","1f76221f0ea84e9ea9b863e58491ac8f","f3cbdfb51f36477d8515c417a8aedb62","4894a567a4eb4b46a07f953521f671d7","944997c41c2e45f4b6e73d53ae67efff"]},"id":"Rh8o9Q5sAUQg","executionInfo":{"status":"ok","timestamp":1634131669060,"user_tz":-120,"elapsed":57892,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"f80c8d91-e3b5-4ba2-fe41-545f15acf5d6"},"source":["dataset_train = load_dataset('imdb', split='train')\n","dataset_test = load_dataset('imdb', split='test')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49075b162cb54745ac39f85387c7fb99","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd5bfc902c1640caacf19fdb70076f45","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a...\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f76221f0ea84e9ea9b863e58491ac8f","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3cbdfb51f36477d8515c417a8aedb62","version_minor":0,"version_major":2},"text/plain":["0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4894a567a4eb4b46a07f953521f671d7","version_minor":0,"version_major":2},"text/plain":["0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"944997c41c2e45f4b6e73d53ae67efff","version_minor":0,"version_major":2},"text/plain":["0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"]},{"output_type":"stream","name":"stdout","text":["Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a. Subsequent calls will reuse this data.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vdn7XlGA7P6","executionInfo":{"status":"ok","timestamp":1634131669061,"user_tz":-120,"elapsed":12,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"54a203e3-5210-4094-9e19-aaec902c8a7c"},"source":["dataset = concatenate_datasets([dataset_train, dataset_test])\n","len(dataset)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50000"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"2oh20wfiNJ_X"},"source":["Now that we have our data, we want to convert it to a DataFrame to facilitate manipulations. In the same time, we convert the strings `neg` to `0` and `pos` to `1` to train our model correctly."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"6982a565","execution_start":1633961469085,"execution_millis":204,"cell_id":"00005-4f991ab7-2294-4ffb-8e57-1d0e9da38da8","deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/","height":203},"id":"0xG776rUAUQu","executionInfo":{"status":"ok","timestamp":1634131669062,"user_tz":-120,"elapsed":10,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"78d7a15b-48b5-432c-c2b6-07de7862e3ba"},"source":["from typing import List, Tuple\n","\n","def create_dataframe(data: List[Tuple[str, str]], columns: List[str]) -> pd.DataFrame:\n","    \"\"\" Convert our data into a DataFrame and convert the string identifier to int \"\"\"\n","\n","    rtn = pd.DataFrame(data, columns=columns)\n","    return rtn\n","\n","df = create_dataframe(list(zip(dataset['label'], dataset['text'])), ['Label', 'Text'])\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Homelessness (or Houselessness as George Carli...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>This is easily the most underrated film inn th...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>This is not the typical Mel Brooks film. It wa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Label                                               Text\n","0      1  Bromwell High is a cartoon comedy. It ran at t...\n","1      1  Homelessness (or Houselessness as George Carli...\n","2      1  Brilliant over-acting by Lesley Ann Warren. Be...\n","3      1  This is easily the most underrated film inn th...\n","4      1  This is not the typical Mel Brooks film. It wa..."]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-a96f0276-9389-46b6-b255-dd5cf77fc165","deepnote_to_be_reexecuted":true,"source_hash":"c5eafed8","execution_start":1633514495899,"execution_millis":1721,"deepnote_cell_type":"code","id":"3tkXVesBAUQv"},"source":["# import packages for steeming\n","import nltk\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.tokenize import word_tokenize "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-82ee19b1-231f-47e5-8b4f-2b4958f70d8b","deepnote_to_be_reexecuted":true,"source_hash":"6344b2ec","execution_start":1633514497635,"execution_millis":484,"deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"6X4-_yxcAUQw","executionInfo":{"status":"ok","timestamp":1634131671051,"user_tz":-120,"elapsed":566,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"297545b6-beb9-41c3-e73a-b4a43d9c2456"},"source":["# We need to download a package for word tokenization\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"kVtFd6_Xy-lE"},"source":["Let's start with the word tokenization."]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00009-6948480c-1089-46b4-8398-5fb8ab5a05ea","deepnote_to_be_reexecuted":true,"source_hash":"8a389644","execution_start":1633514498112,"execution_millis":72403,"deepnote_cell_type":"code","id":"hV9u-UKTAUQx"},"source":["# Tokenization\n","df['Text'] = df['Text'].apply(lambda x: \" \".join(word_tokenize(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rulCFVaBzVtl"},"source":["Now let's apply the stemming to everything that is composed of characters. Words are simply cut and stemmed. We do not have any punctuation."]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00009-c13f0583-67e2-46a0-83c7-a7802cbda751","deepnote_to_be_reexecuted":true,"source_hash":"4420383e","execution_start":1633514570535,"execution_millis":195586,"deepnote_cell_type":"code","id":"vatRTIADAUQy"},"source":["# Steeming\n","import re\n","\n","re_word = re.compile(r\"^\\w+$\")\n","stemmer = SnowballStemmer(\"english\")\n","\n","def stemming(text):\n","    return [stemmer.stem(word) for word in word_tokenize(text.lower()) if re_word.match(word)]\n","        \n","df['Text'] = df['Text'].apply(lambda x: \" \".join(stemming(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-9684ad1f-f487-451b-ab9b-bc33a27d173a","deepnote_to_be_reexecuted":true,"source_hash":"17b1f6eb","execution_start":1633514766164,"execution_millis":4,"deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"ikwmrjPQAUQy","executionInfo":{"status":"ok","timestamp":1634131998662,"user_tz":-120,"elapsed":11,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"1634ce55-46d8-4a1e-9a93-d04695b6a448"},"source":["df['Text'][:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    bromwel high is a cartoon comedi it ran at the...\n","1    homeless or houseless as georg carlin state ha...\n","2    brilliant by lesley ann warren best dramat hob...\n","3    this is easili the most underr film inn the br...\n","4    this is not the typic mel brook film it was mu...\n","Name: Text, dtype: object"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-87174b45-3a91-4a30-abf7-399ca5ed6b8f","deepnote_to_be_reexecuted":true,"source_hash":"5a834a00","execution_start":1633514856261,"execution_millis":14474,"deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"fV2S5_dYAUQz","executionInfo":{"status":"ok","timestamp":1634132004772,"user_tz":-120,"elapsed":6115,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"4e1bd603-6767-4dcf-94b3-a4b1302e27d5"},"source":["!python -m spacy download en_core_web_sm"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 26.6 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"markdown","metadata":{"id":"lujEKaPMzi0h"},"source":["\n","Let's lemmatize all token we can find."]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-a3265a7a-d67c-48dc-8d43-a39d4981416b","deepnote_to_be_reexecuted":true,"source_hash":"4d95fdb2","execution_start":1633514873966,"execution_millis":1115785,"deepnote_cell_type":"code","id":"0OtDlQFQAUQ-"},"source":["# Lemmatization\n","import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\", disable = ['ner', 'tagger', 'parser', 'textcat', \"lemmatizer\"])\n","\n","def lemmatization(text):\n","    return [token.lemma_ for token in nlp(text.lower()) if re_word.match(token.text)]\n","\n","df['Text'] = df['Text'].apply(lambda x: \" \".join(lemmatization(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00013-009a3e1b-61c7-405e-9cf1-2b33cb4370cc","deepnote_to_be_reexecuted":true,"source_hash":"17b1f6eb","execution_start":1633516047176,"execution_millis":10,"deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"McIcj3mKAURA","executionInfo":{"status":"ok","timestamp":1634132069854,"user_tz":-120,"elapsed":22,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"34694b59-6d93-4a11-e2e6-539bab54a6d9"},"source":["df['Text'][:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    bromwel high be a cartoon comedi it run at the...\n","1    homeless or houseless a georg carlin state hav...\n","2    brilliant by lesley ann warren well dramat hob...\n","3    this be easili the much underr film inn the br...\n","4    this be not the typic mel brook film it be muc...\n","Name: Text, dtype: object"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"ID7r7PVmztnd"},"source":["For exemple, in the first sentence we can see that 'is' has been transformed by 'be' and 'ran' by 'run'."]},{"cell_type":"markdown","metadata":{"id":"Igk59ayFNROS"},"source":["First, we need to convert the text into numbers that we can do calculations on. We use word frequencies. We want to transform the given text to a vector on the basis of the frequency of each word in the text.\n","\n","For this we use `CountVectorizer` from `sklearn`. "]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"cca4b111","execution_start":1633961469279,"execution_millis":5851,"cell_id":"00006-4348df38-572d-45da-8ae6-9b450809996d","deepnote_cell_type":"code","id":"szCM6YPQAURi"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer(max_features = 1500)\n"," \n","X = cv.fit_transform(df['Text']).toarray()\n","y = df['Label']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJqzZYkGNV9I"},"source":["The `train_test_split` shuffles all the dataset before splitting. In our case, we will use 75% of data for training and 25% for testing."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"6d51c904","execution_start":1633961475138,"execution_millis":196,"cell_id":"00007-b7dd27d5-4a59-4d1a-8dd6-d2758548b087","deepnote_cell_type":"code","id":"xy3vscoVAURj"},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(\n","           X, y, test_size = 0.25, random_state = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4WLLNEP5VCcY"},"source":["Bayes Theorem describes for two independent events `A` and `B` that: \n","$$ P(A_B) = (P(B_A) * P(A))/P(B) $$\n","\n","We're going to use the Naive Bayes Classifier Algorithm based on applying Bayes' theorem. Here, we assume the `naive` condition that every word in a sentence is independent of the other ones. This means that now we look at individual words. So for example: \n","$$ P(\\text{liked the movie}) = P(\\text{liked}) * P(\\text{the}) * P(\\text{movie}) $$"]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"4b453199","execution_start":1633516018557,"execution_millis":597,"cell_id":"00008-16a44e6e-66cc-46ad-a016-8c54b7af00ff","deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"jLDGD3S2AUSC","executionInfo":{"status":"ok","timestamp":1634132079880,"user_tz":-120,"elapsed":862,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"534b2c42-5b29-46e1-eaa5-ca5623b1c649"},"source":["from sklearn.naive_bayes import GaussianNB\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GaussianNB(priors=None, var_smoothing=1e-09)"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"YVCJLw018kyw"},"source":["We use the confusion_matrix of sklearn to display the number of right (True positive and True negative) and wrong (False positive and False negative) predictions."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"c219da1f","execution_start":1633516021570,"execution_millis":127,"cell_id":"00009-2906fec9-1e78-4189-b1af-e19e61e53365","deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"AMlXzmQQAUSE","executionInfo":{"status":"ok","timestamp":1634132080240,"user_tz":-120,"elapsed":362,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"dd6fba6b-f828-4fe1-e93e-9be1ad621d50"},"source":["from sklearn.metrics import confusion_matrix\n","y_pred = gnb.predict(X_test)\n","cm = confusion_matrix(y_test, y_pred)\n","cm"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[5315,  937],\n","       [2235, 4013]])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"F7RpfpoA8n-8"},"source":["We use the classification_report of sklearn to display the precision, recall, and F1-score for both classes on the test data."]},{"cell_type":"code","metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"aae199db","execution_start":1633516024615,"execution_millis":24,"cell_id":"00010-a8ef51dc-f3a6-454d-8ef4-0508ae2a1951","deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"LQc0XeitAUSF","executionInfo":{"status":"ok","timestamp":1634132080241,"user_tz":-120,"elapsed":12,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"63a446d2-e7b9-4780-c75b-4916cc0e3de9"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.70      0.85      0.77      6252\n","           1       0.81      0.64      0.72      6248\n","\n","    accuracy                           0.75     12500\n","   macro avg       0.76      0.75      0.74     12500\n","weighted avg       0.76      0.75      0.74     12500\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"uOzsO6c00NST"},"source":["If we compare those results with the naive bayers without pretreatement, we can observe that the accucary, precision, recall, f1-score for both classes is less good. In that case, pretreatement is quite useless because naive bayers learn each word independently"]},{"cell_type":"markdown","metadata":{"id":"g80mMGvR0CBc"},"source":["Now we want to see which samples have been wrongly classified"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"ZyCuMbQAz9SX","executionInfo":{"status":"ok","timestamp":1634132080242,"user_tz":-120,"elapsed":11,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"e9851529-3301-4dc0-c11e-a21ba8e2c8f4"},"source":["bad_predict_df = y_test.where(y_test != y_pred).dropna()\n","indexes = bad_predict_df.index\n","df.iloc[indexes]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>45519</th>\n","      <td>0</td>\n","      <td>i think the movi be one side i watch it recent...</td>\n","    </tr>\n","    <tr>\n","      <th>26128</th>\n","      <td>1</td>\n","      <td>i realli like this pictur becaus it realist de...</td>\n","    </tr>\n","    <tr>\n","      <th>26376</th>\n","      <td>1</td>\n","      <td>i think it be a brilliant show with cool talk ...</td>\n","    </tr>\n","    <tr>\n","      <th>32104</th>\n","      <td>1</td>\n","      <td>what a refresh chang from the pg movi that hav...</td>\n","    </tr>\n","    <tr>\n","      <th>43460</th>\n","      <td>0</td>\n","      <td>ray bradburi run and hide this tacki film vers...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9769</th>\n","      <td>1</td>\n","      <td>this be a visual adapt of manga with veri litt...</td>\n","    </tr>\n","    <tr>\n","      <th>27615</th>\n","      <td>1</td>\n","      <td>sheba babi be alway underr much like becaus it...</td>\n","    </tr>\n","    <tr>\n","      <th>35021</th>\n","      <td>1</td>\n","      <td>this film be fill with great act great music s...</td>\n","    </tr>\n","    <tr>\n","      <th>5420</th>\n","      <td>1</td>\n","      <td>rooki be a wonder movi about the 2 chanc life ...</td>\n","    </tr>\n","    <tr>\n","      <th>30528</th>\n","      <td>1</td>\n","      <td>good i guess i be in the mood for a movi that ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3172 rows × 2 columns</p>\n","</div>"],"text/plain":["       Label                                               Text\n","45519      0  i think the movi be one side i watch it recent...\n","26128      1  i realli like this pictur becaus it realist de...\n","26376      1  i think it be a brilliant show with cool talk ...\n","32104      1  what a refresh chang from the pg movi that hav...\n","43460      0  ray bradburi run and hide this tacki film vers...\n","...      ...                                                ...\n","9769       1  this be a visual adapt of manga with veri litt...\n","27615      1  sheba babi be alway underr much like becaus it...\n","35021      1  this film be fill with great act great music s...\n","5420       1  rooki be a wonder movi about the 2 chanc life ...\n","30528      1  good i guess i be in the mood for a movi that ...\n","\n","[3172 rows x 2 columns]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"xGdD7FJK7NU7","executionInfo":{"status":"ok","timestamp":1634133154972,"user_tz":-120,"elapsed":246,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"b7a5bd26-5245-44fd-cd5c-badadbb93ea3"},"source":["df.iloc[32104][\"Text\"]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'what a refresh chang from the pg movi that have teen girl jump in and out of bed young high school boy count how mani girl they can hook up with kid drink do drug etc etc etc carl hiaasen have write so mani book that be enjoy but hard classic literatur but he have final write someth that middl school kid want to read and this movi send a messag to kid that mayb they can make a differ that mayb their voic can be hear film in south florida the sceneri be beauti and natur and real who care if it predict and a littl corni so be free willi and look how good that do this be a good famili movi rare breed'"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"z2IEidt_7RRR","executionInfo":{"status":"ok","timestamp":1634133156557,"user_tz":-120,"elapsed":204,"user":{"displayName":"Kevin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13559605239732872861"}},"outputId":"780c9e1a-0c09-4f79-952c-c9fcecd40224"},"source":["df.iloc[26128][\"Text\"]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'i realli like this pictur becaus it realist deal with two peopl in love and one of them have a disord though the end sadden me i know that that be the well way for it to finish off i would recom this to everyon'"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"AmCnUsHY7S2e"},"source":["Like the naive bayers without pretreatement, it is quite the same wrongly classified sentences for same problems. "]}]}